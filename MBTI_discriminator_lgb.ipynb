{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-67fd75df1624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import *\n",
    "import seaborn as sns\n",
    "from nltk.corpus import nps_chat\n",
    "from nltk.corpus import webtext\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "import pickle\n",
    "import nltk\n",
    "import itertools\n",
    "import sklearn\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_3=4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Light GBM to Classify Kaggle MBTI DataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer,CountVectorizer\n",
    "from scipy.sparse import hstack,csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "lab_encoder = LabelEncoder().fit(unique_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8675\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../../data/mbti_1.csv')\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 720 µs\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 380 µs\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.54 ms\n",
      "\n",
      "Before preprocessing:\n",
      "\n",
      " 'I'm finding the lack of me in these posts very alarming.|||Sex can be boring if it's in the same position often. For example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary. There isn't enough...|||Giving new meaning to 'Game' theory.|||Hello *ENTP Grin*  That's all it takes. Than we converse and they do most of the flirting while I acknowledge their presence and return their words with smooth wordplay and more cheeky grins.|||This + L\n",
      "\n",
      "After preprocessing:\n",
      "\n",
      "  i m finding the lack of me in these posts very alarming sex can be boring if it s in the same position often for example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary there isn t enough giving new meaning to game theory hello entp grin that s all it takes than we converse and they do most of the flirting while i acknowledge their presence and return their words with smooth wordplay and more cheeky grins this lack of balance and hand \n",
      "\n",
      "List of urls:\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "##### Compute list of subject with Type | list of comments \n",
    "# Time\n",
    "%time data.posts[1].replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n",
    "%time re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', data.posts[1])\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer | Stemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Cache the stop words for speed \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "# One post\n",
    "OnePost = data.posts[1]\n",
    "# List all urls\n",
    "urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', OnePost)\n",
    "# Remove urls\n",
    "temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', OnePost)\n",
    "# Keep only words\n",
    "temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "# Remove spaces > 1\n",
    "temp = re.sub(' +', ' ', temp).lower()\n",
    "# Remove stopwords and lematize\n",
    "%time stemmer.stem(\" \".join([w for w in temp.split(' ') if w not in cachedStopWords]))\n",
    "\n",
    "print(\"\\nBefore preprocessing:\\n\\n\", OnePost[0:500])\n",
    "print(\"\\nAfter preprocessing:\\n\\n\", temp[0:500])\n",
    "print(\"\\nList of urls:\")\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 | 8675 rows\n",
      "1000 | 8675 rows\n",
      "1500 | 8675 rows\n",
      "2000 | 8675 rows\n",
      "2500 | 8675 rows\n",
      "3000 | 8675 rows\n",
      "3500 | 8675 rows\n",
      "4000 | 8675 rows\n",
      "4500 | 8675 rows\n",
      "5000 | 8675 rows\n",
      "5500 | 8675 rows\n",
      "6000 | 8675 rows\n",
      "6500 | 8675 rows\n",
      "7000 | 8675 rows\n",
      "7500 | 8675 rows\n",
      "8000 | 8675 rows\n",
      "8500 | 8675 rows\n"
     ]
    }
   ],
   "source": [
    "##### Compute list of subject with Type | list of comments \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Lemmatize\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "def pre_process_data(data, remove_stop_words=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    list_type=[]\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if i % 500 == 0:\n",
    "            print(\"%s | %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', posts)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp).lower()\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "\n",
    "        type_labelized = lab_encoder.transform([row[1].type])[0]\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "        list_type.append(row[1].type)\n",
    "\n",
    "    #del data\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality,list_type\n",
    "\n",
    "list_posts, list_personality,list_type = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>length</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>TF</th>\n",
       "      <th>JP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>link link enfp intj moment link sportscenter ...</td>\n",
       "      <td>8</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>2156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>finding lack post alarming sex boring positio...</td>\n",
       "      <td>3</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>3606</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good one link course say know blessing curse ...</td>\n",
       "      <td>11</td>\n",
       "      <td>INTP</td>\n",
       "      <td>2883</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear intp enjoyed conversation day esoteric g...</td>\n",
       "      <td>10</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>3501</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fired another silly misconception approaching...</td>\n",
       "      <td>2</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>3253</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  label  type  length  IE  \\\n",
       "0   link link enfp intj moment link sportscenter ...      8  INFJ    2156   0   \n",
       "1   finding lack post alarming sex boring positio...      3  ENTP    3606   1   \n",
       "2   good one link course say know blessing curse ...     11  INTP    2883   0   \n",
       "3   dear intp enjoyed conversation day esoteric g...     10  INTJ    3501   0   \n",
       "4   fired another silly misconception approaching...      2  ENTJ    3253   1   \n",
       "\n",
       "   NS  TF  JP  \n",
       "0   0   1   0  \n",
       "1   0   0   1  \n",
       "2   0   0   1  \n",
       "3   0   0   0  \n",
       "4   0   0   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_data=pd.DataFrame({'content':list_posts,'label':list_personality,'type':list_type})\n",
    "prep_data['length']=prep_data['content'].apply(lambda x:len(x))\n",
    "prep_data['IE']=prep_data['type'].apply(lambda x:int(x[0]!='I'))\n",
    "prep_data['NS']=prep_data['type'].apply(lambda x:int(x[1]!='N'))\n",
    "prep_data['TF']=prep_data['type'].apply(lambda x:int(x[2]!='T'))\n",
    "prep_data['JP']=prep_data['type'].apply(lambda x:int(x[3]!='J'))\n",
    "print(prep_data.shape)\n",
    "prep_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7807, 8)\n"
     ]
    }
   ],
   "source": [
    "train, test=train_test_split(prep_data,test_size=0.1)\n",
    "#train=prep_data\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7807, 55443)\n",
      "TFIDF FINISHED...\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,4),analyzer='char',stop_words='english', max_df=0.9, min_df=5)\n",
    "train_X_tfidf = tfidf.fit_transform(train.content)\n",
    "print(train_X_tfidf.shape)\n",
    "print('TFIDF FINISHED...')\n",
    "test_X_tfidf = tfidf.transform(test.content) #43980\n",
    "#joblib.dump(tfidf,'lgb/tfidf.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7807, 16384)\n",
      "hstack finished\n"
     ]
    }
   ],
   "source": [
    "ha = HashingVectorizer(ngram_range=(1,2),lowercase=False,n_features=2**14)\n",
    "train_X_ha = ha.fit_transform(train.content)\n",
    "print(train_X_ha.shape)\n",
    "test_X_ha = ha.transform(test.content)\n",
    "#joblib.dump(ha,'lgb/ha.pkl')\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "#train_len=mm.fit_transform(train.length.values.reshape(-1,1))\n",
    "#test_len=mm.transform(test.length.values.reshape(-1,1))\n",
    "#del mm\n",
    "\n",
    "train_X=hstack([train_X_tfidf,train_X_ha]) #csr_matrix(train_len)\n",
    "test_X=hstack([test_X_tfidf,test_X_ha]) #csr_matrix(test_len)\n",
    "\n",
    "print(\"hstack finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7807, 71827) (868, 71827)\n",
      "(7807, 4) (868, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_col=['IE','NS','TF','JP']\n",
    "train_Y = train[label_col].values\n",
    "test_Y = test[label_col].values\n",
    "print(train_X.shape,test_X.shape)\n",
    "print(train_Y.shape,test_Y.shape)\n",
    "train_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_error: 0.231203\tvalid_1's binary_error: 0.223502\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\ttraining's binary_error: 0.231203\tvalid_1's binary_error: 0.223502\n",
      "[3]\ttraining's binary_error: 0.231203\tvalid_1's binary_error: 0.223502\n",
      "[4]\ttraining's binary_error: 0.231203\tvalid_1's binary_error: 0.223502\n",
      "[5]\ttraining's binary_error: 0.230178\tvalid_1's binary_error: 0.223502\n",
      "[6]\ttraining's binary_error: 0.166005\tvalid_1's binary_error: 0.177419\n",
      "[7]\ttraining's binary_error: 0.144614\tvalid_1's binary_error: 0.15553\n",
      "[8]\ttraining's binary_error: 0.134367\tvalid_1's binary_error: 0.147465\n",
      "[9]\ttraining's binary_error: 0.123607\tvalid_1's binary_error: 0.144009\n",
      "[10]\ttraining's binary_error: 0.117843\tvalid_1's binary_error: 0.142857\n",
      "[11]\ttraining's binary_error: 0.115665\tvalid_1's binary_error: 0.140553\n",
      "[12]\ttraining's binary_error: 0.111054\tvalid_1's binary_error: 0.139401\n",
      "[13]\ttraining's binary_error: 0.106699\tvalid_1's binary_error: 0.138249\n",
      "[14]\ttraining's binary_error: 0.105162\tvalid_1's binary_error: 0.134793\n",
      "[15]\ttraining's binary_error: 0.100935\tvalid_1's binary_error: 0.135945\n",
      "[16]\ttraining's binary_error: 0.0995261\tvalid_1's binary_error: 0.135945\n",
      "[17]\ttraining's binary_error: 0.0972204\tvalid_1's binary_error: 0.133641\n",
      "[18]\ttraining's binary_error: 0.0940182\tvalid_1's binary_error: 0.134793\n",
      "[19]\ttraining's binary_error: 0.0935058\tvalid_1's binary_error: 0.130184\n",
      "[20]\ttraining's binary_error: 0.0901755\tvalid_1's binary_error: 0.129032\n",
      "[21]\ttraining's binary_error: 0.0874856\tvalid_1's binary_error: 0.134793\n",
      "[22]\ttraining's binary_error: 0.0850519\tvalid_1's binary_error: 0.133641\n",
      "[23]\ttraining's binary_error: 0.0832586\tvalid_1's binary_error: 0.134793\n",
      "[24]\ttraining's binary_error: 0.0798002\tvalid_1's binary_error: 0.133641\n",
      "[25]\ttraining's binary_error: 0.0769822\tvalid_1's binary_error: 0.130184\n",
      "[26]\ttraining's binary_error: 0.0754451\tvalid_1's binary_error: 0.12788\n",
      "[27]\ttraining's binary_error: 0.073908\tvalid_1's binary_error: 0.130184\n",
      "[28]\ttraining's binary_error: 0.0712181\tvalid_1's binary_error: 0.130184\n",
      "[29]\ttraining's binary_error: 0.0682721\tvalid_1's binary_error: 0.129032\n",
      "[30]\ttraining's binary_error: 0.0654541\tvalid_1's binary_error: 0.125576\n",
      "[31]\ttraining's binary_error: 0.0644294\tvalid_1's binary_error: 0.126728\n",
      "[32]\ttraining's binary_error: 0.0618676\tvalid_1's binary_error: 0.126728\n",
      "[33]\ttraining's binary_error: 0.0598181\tvalid_1's binary_error: 0.129032\n",
      "[34]\ttraining's binary_error: 0.0589215\tvalid_1's binary_error: 0.12788\n",
      "[35]\ttraining's binary_error: 0.0545664\tvalid_1's binary_error: 0.126728\n",
      "[36]\ttraining's binary_error: 0.0535417\tvalid_1's binary_error: 0.125576\n",
      "[37]\ttraining's binary_error: 0.0503394\tvalid_1's binary_error: 0.125576\n",
      "[38]\ttraining's binary_error: 0.0500833\tvalid_1's binary_error: 0.12788\n",
      "[39]\ttraining's binary_error: 0.0450877\tvalid_1's binary_error: 0.126728\n",
      "[40]\ttraining's binary_error: 0.0432945\tvalid_1's binary_error: 0.126728\n",
      "[41]\ttraining's binary_error: 0.0418855\tvalid_1's binary_error: 0.130184\n",
      "[42]\ttraining's binary_error: 0.0402203\tvalid_1's binary_error: 0.129032\n",
      "[43]\ttraining's binary_error: 0.0377866\tvalid_1's binary_error: 0.129032\n",
      "[44]\ttraining's binary_error: 0.0357372\tvalid_1's binary_error: 0.129032\n",
      "[45]\ttraining's binary_error: 0.0336877\tvalid_1's binary_error: 0.12788\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's binary_error: 0.0654541\tvalid_1's binary_error: 0.125576\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I/E classifier\n",
    "'''\n",
    "label_index=0\n",
    "d_train = lgb.Dataset(train_X,train_Y[:,label_index])\n",
    "d_valid = lgb.Dataset(test_X,test_Y[:,label_index])\n",
    "\n",
    "watch_list = [d_train,d_valid]\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',\n",
    "        #'num_class' : 16,\n",
    "        'num_leaves': 32,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'nthread': 4\n",
    "    }\n",
    "#evals_result={}\n",
    "model1 = lgb.train(params, d_train, 100, watch_list, early_stopping_rounds=15,verbose_eval=1) #25\n",
    "#online_model1 = lgb.train(params,lgb.Dataset(train_X,train_Y[:,label_index]),num_boost_round=30) # 这里写个300-400效果更好，为了速度问题写的100 # 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868,)\n",
      "0.6784660766961651\n",
      "0.8781164917862277\n"
     ]
    }
   ],
   "source": [
    "preds=model1.predict(test_X)\n",
    "print(preds.shape)\n",
    "print(f1_score(test_Y[:,label_index],preds>0.5))\n",
    "print(roc_auc_score(test_Y[:,label_index],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_error: 0.138081\tvalid_1's binary_error: 0.137097\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\ttraining's binary_error: 0.138081\tvalid_1's binary_error: 0.137097\n",
      "[3]\ttraining's binary_error: 0.138081\tvalid_1's binary_error: 0.137097\n",
      "[4]\ttraining's binary_error: 0.138081\tvalid_1's binary_error: 0.137097\n",
      "[5]\ttraining's binary_error: 0.137569\tvalid_1's binary_error: 0.137097\n",
      "[6]\ttraining's binary_error: 0.115025\tvalid_1's binary_error: 0.117512\n",
      "[7]\ttraining's binary_error: 0.100551\tvalid_1's binary_error: 0.114055\n",
      "[8]\ttraining's binary_error: 0.0913283\tvalid_1's binary_error: 0.109447\n",
      "[9]\ttraining's binary_error: 0.0855642\tvalid_1's binary_error: 0.102535\n",
      "[10]\ttraining's binary_error: 0.0803125\tvalid_1's binary_error: 0.0979263\n",
      "[11]\ttraining's binary_error: 0.0749328\tvalid_1's binary_error: 0.09447\n",
      "[12]\ttraining's binary_error: 0.0714743\tvalid_1's binary_error: 0.0967742\n",
      "[13]\ttraining's binary_error: 0.0676316\tvalid_1's binary_error: 0.0956221\n",
      "[14]\ttraining's binary_error: 0.0644294\tvalid_1's binary_error: 0.093318\n",
      "[15]\ttraining's binary_error: 0.0603305\tvalid_1's binary_error: 0.0910138\n",
      "[16]\ttraining's binary_error: 0.0580248\tvalid_1's binary_error: 0.0921659\n",
      "[17]\ttraining's binary_error: 0.0557192\tvalid_1's binary_error: 0.0921659\n",
      "[18]\ttraining's binary_error: 0.0529012\tvalid_1's binary_error: 0.0910138\n",
      "[19]\ttraining's binary_error: 0.051108\tvalid_1's binary_error: 0.0887097\n",
      "[20]\ttraining's binary_error: 0.0480338\tvalid_1's binary_error: 0.0898618\n",
      "[21]\ttraining's binary_error: 0.0466248\tvalid_1's binary_error: 0.0910138\n",
      "[22]\ttraining's binary_error: 0.0457282\tvalid_1's binary_error: 0.0898618\n",
      "[23]\ttraining's binary_error: 0.0434226\tvalid_1's binary_error: 0.0910138\n",
      "[24]\ttraining's binary_error: 0.0420136\tvalid_1's binary_error: 0.0921659\n",
      "[25]\ttraining's binary_error: 0.0406046\tvalid_1's binary_error: 0.0910138\n",
      "[26]\ttraining's binary_error: 0.0385551\tvalid_1's binary_error: 0.0921659\n",
      "[27]\ttraining's binary_error: 0.0379147\tvalid_1's binary_error: 0.0921659\n",
      "[28]\ttraining's binary_error: 0.0361214\tvalid_1's binary_error: 0.0898618\n",
      "[29]\ttraining's binary_error: 0.0334315\tvalid_1's binary_error: 0.0898618\n",
      "[30]\ttraining's binary_error: 0.0309978\tvalid_1's binary_error: 0.0875576\n",
      "[31]\ttraining's binary_error: 0.0294607\tvalid_1's binary_error: 0.0921659\n",
      "[32]\ttraining's binary_error: 0.0271551\tvalid_1's binary_error: 0.0910138\n",
      "[33]\ttraining's binary_error: 0.0262585\tvalid_1's binary_error: 0.0898618\n",
      "[34]\ttraining's binary_error: 0.024209\tvalid_1's binary_error: 0.0875576\n",
      "[35]\ttraining's binary_error: 0.0233124\tvalid_1's binary_error: 0.0898618\n",
      "[36]\ttraining's binary_error: 0.021263\tvalid_1's binary_error: 0.0898618\n",
      "[37]\ttraining's binary_error: 0.0188293\tvalid_1's binary_error: 0.0910138\n",
      "[38]\ttraining's binary_error: 0.0175484\tvalid_1's binary_error: 0.0921659\n",
      "[39]\ttraining's binary_error: 0.0161394\tvalid_1's binary_error: 0.0910138\n",
      "[40]\ttraining's binary_error: 0.0143461\tvalid_1's binary_error: 0.0921659\n",
      "[41]\ttraining's binary_error: 0.0121686\tvalid_1's binary_error: 0.0898618\n",
      "[42]\ttraining's binary_error: 0.0111438\tvalid_1's binary_error: 0.0898618\n",
      "[43]\ttraining's binary_error: 0.0105034\tvalid_1's binary_error: 0.0910138\n",
      "[44]\ttraining's binary_error: 0.00871013\tvalid_1's binary_error: 0.0921659\n",
      "[45]\ttraining's binary_error: 0.00845395\tvalid_1's binary_error: 0.09447\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttraining's binary_error: 0.0309978\tvalid_1's binary_error: 0.0875576\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "N/S Classifier\n",
    "'''\n",
    "label_index=1\n",
    "d_train = lgb.Dataset(train_X,train_Y[:,label_index])\n",
    "d_valid = lgb.Dataset(test_X,test_Y[:,label_index])\n",
    "\n",
    "watch_list = [d_train,d_valid]\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',\n",
    "        #'num_class' : 16,\n",
    "        'num_leaves': 32,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'nthread': 4\n",
    "    }\n",
    "#evals_result={}\n",
    "model2 = lgb.train(params, d_train, 100, watch_list, early_stopping_rounds=15,verbose_eval=1) #40\n",
    "#online_model2 = lgb.train(params,lgb.Dataset(train_X,train_Y[:,label_index]),num_boost_round=43) # 这里写个300-400效果更好，为了速度问题写的100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868,)\n",
      "0.5869565217391304\n",
      "0.9088981386947302\n"
     ]
    }
   ],
   "source": [
    "preds=model2.predict(test_X)\n",
    "print(preds.shape)\n",
    "print(f1_score(test_Y[:,label_index],preds>0.5))\n",
    "print(roc_auc_score(test_Y[:,label_index],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_error: 0.251953\tvalid_1's binary_error: 0.267281\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\ttraining's binary_error: 0.176636\tvalid_1's binary_error: 0.207373\n",
      "[3]\ttraining's binary_error: 0.154605\tvalid_1's binary_error: 0.199309\n",
      "[4]\ttraining's binary_error: 0.150378\tvalid_1's binary_error: 0.191244\n",
      "[5]\ttraining's binary_error: 0.144742\tvalid_1's binary_error: 0.172811\n",
      "[6]\ttraining's binary_error: 0.138978\tvalid_1's binary_error: 0.169355\n",
      "[7]\ttraining's binary_error: 0.135263\tvalid_1's binary_error: 0.176267\n",
      "[8]\ttraining's binary_error: 0.13078\tvalid_1's binary_error: 0.178571\n",
      "[9]\ttraining's binary_error: 0.12809\tvalid_1's binary_error: 0.177419\n",
      "[10]\ttraining's binary_error: 0.123223\tvalid_1's binary_error: 0.176267\n",
      "[11]\ttraining's binary_error: 0.120277\tvalid_1's binary_error: 0.171659\n",
      "[12]\ttraining's binary_error: 0.118227\tvalid_1's binary_error: 0.176267\n",
      "[13]\ttraining's binary_error: 0.114641\tvalid_1's binary_error: 0.170507\n",
      "[14]\ttraining's binary_error: 0.112719\tvalid_1's binary_error: 0.168203\n",
      "[15]\ttraining's binary_error: 0.10862\tvalid_1's binary_error: 0.165899\n",
      "[16]\ttraining's binary_error: 0.107852\tvalid_1's binary_error: 0.168203\n",
      "[17]\ttraining's binary_error: 0.105034\tvalid_1's binary_error: 0.164747\n",
      "[18]\ttraining's binary_error: 0.103113\tvalid_1's binary_error: 0.169355\n",
      "[19]\ttraining's binary_error: 0.100423\tvalid_1's binary_error: 0.164747\n",
      "[20]\ttraining's binary_error: 0.0981171\tvalid_1's binary_error: 0.165899\n",
      "[21]\ttraining's binary_error: 0.0961957\tvalid_1's binary_error: 0.164747\n",
      "[22]\ttraining's binary_error: 0.0942744\tvalid_1's binary_error: 0.164747\n",
      "[23]\ttraining's binary_error: 0.0922249\tvalid_1's binary_error: 0.168203\n",
      "[24]\ttraining's binary_error: 0.0903036\tvalid_1's binary_error: 0.165899\n",
      "[25]\ttraining's binary_error: 0.0885103\tvalid_1's binary_error: 0.168203\n",
      "[26]\ttraining's binary_error: 0.0854361\tvalid_1's binary_error: 0.164747\n",
      "[27]\ttraining's binary_error: 0.0835148\tvalid_1's binary_error: 0.170507\n",
      "[28]\ttraining's binary_error: 0.0800564\tvalid_1's binary_error: 0.164747\n",
      "[29]\ttraining's binary_error: 0.0783912\tvalid_1's binary_error: 0.163594\n",
      "[30]\ttraining's binary_error: 0.075317\tvalid_1's binary_error: 0.165899\n",
      "[31]\ttraining's binary_error: 0.072499\tvalid_1's binary_error: 0.162442\n",
      "[32]\ttraining's binary_error: 0.0703215\tvalid_1's binary_error: 0.162442\n",
      "[33]\ttraining's binary_error: 0.0685282\tvalid_1's binary_error: 0.160138\n",
      "[34]\ttraining's binary_error: 0.0672473\tvalid_1's binary_error: 0.167051\n",
      "[35]\ttraining's binary_error: 0.0641732\tvalid_1's binary_error: 0.16129\n",
      "[36]\ttraining's binary_error: 0.0619956\tvalid_1's binary_error: 0.162442\n",
      "[37]\ttraining's binary_error: 0.0599462\tvalid_1's binary_error: 0.158986\n",
      "[38]\ttraining's binary_error: 0.0577687\tvalid_1's binary_error: 0.16129\n",
      "[39]\ttraining's binary_error: 0.0545664\tvalid_1's binary_error: 0.158986\n",
      "[40]\ttraining's binary_error: 0.0541821\tvalid_1's binary_error: 0.160138\n",
      "[41]\ttraining's binary_error: 0.0521327\tvalid_1's binary_error: 0.157834\n",
      "[42]\ttraining's binary_error: 0.0494428\tvalid_1's binary_error: 0.160138\n",
      "[43]\ttraining's binary_error: 0.0475215\tvalid_1's binary_error: 0.158986\n",
      "[44]\ttraining's binary_error: 0.0457282\tvalid_1's binary_error: 0.162442\n",
      "[45]\ttraining's binary_error: 0.0439349\tvalid_1's binary_error: 0.158986\n",
      "[46]\ttraining's binary_error: 0.0422698\tvalid_1's binary_error: 0.153226\n",
      "[47]\ttraining's binary_error: 0.0411169\tvalid_1's binary_error: 0.15553\n",
      "[48]\ttraining's binary_error: 0.0391956\tvalid_1's binary_error: 0.153226\n",
      "[49]\ttraining's binary_error: 0.0370181\tvalid_1's binary_error: 0.153226\n",
      "[50]\ttraining's binary_error: 0.0347124\tvalid_1's binary_error: 0.150922\n",
      "[51]\ttraining's binary_error: 0.0333034\tvalid_1's binary_error: 0.147465\n",
      "[52]\ttraining's binary_error: 0.0320225\tvalid_1's binary_error: 0.14977\n",
      "[53]\ttraining's binary_error: 0.031254\tvalid_1's binary_error: 0.146313\n",
      "[54]\ttraining's binary_error: 0.0295888\tvalid_1's binary_error: 0.148618\n",
      "[55]\ttraining's binary_error: 0.0285641\tvalid_1's binary_error: 0.148618\n",
      "[56]\ttraining's binary_error: 0.0281798\tvalid_1's binary_error: 0.146313\n",
      "[57]\ttraining's binary_error: 0.0257461\tvalid_1's binary_error: 0.147465\n",
      "[58]\ttraining's binary_error: 0.0249776\tvalid_1's binary_error: 0.146313\n",
      "[59]\ttraining's binary_error: 0.0244652\tvalid_1's binary_error: 0.148618\n",
      "[60]\ttraining's binary_error: 0.0230562\tvalid_1's binary_error: 0.146313\n",
      "[61]\ttraining's binary_error: 0.0220315\tvalid_1's binary_error: 0.147465\n",
      "[62]\ttraining's binary_error: 0.021263\tvalid_1's binary_error: 0.148618\n",
      "[63]\ttraining's binary_error: 0.0187012\tvalid_1's binary_error: 0.145161\n",
      "[64]\ttraining's binary_error: 0.0172922\tvalid_1's binary_error: 0.148618\n",
      "[65]\ttraining's binary_error: 0.0167798\tvalid_1's binary_error: 0.148618\n",
      "[66]\ttraining's binary_error: 0.0165236\tvalid_1's binary_error: 0.146313\n",
      "[67]\ttraining's binary_error: 0.0158832\tvalid_1's binary_error: 0.148618\n",
      "[68]\ttraining's binary_error: 0.0147304\tvalid_1's binary_error: 0.148618\n",
      "[69]\ttraining's binary_error: 0.0137056\tvalid_1's binary_error: 0.14977\n",
      "[70]\ttraining's binary_error: 0.0130652\tvalid_1's binary_error: 0.147465\n",
      "[71]\ttraining's binary_error: 0.0117843\tvalid_1's binary_error: 0.14977\n",
      "[72]\ttraining's binary_error: 0.0103753\tvalid_1's binary_error: 0.150922\n",
      "[73]\ttraining's binary_error: 0.0105034\tvalid_1's binary_error: 0.152074\n",
      "[74]\ttraining's binary_error: 0.00896631\tvalid_1's binary_error: 0.150922\n",
      "[75]\ttraining's binary_error: 0.00806968\tvalid_1's binary_error: 0.14977\n",
      "[76]\ttraining's binary_error: 0.00627642\tvalid_1's binary_error: 0.147465\n",
      "[77]\ttraining's binary_error: 0.00499552\tvalid_1's binary_error: 0.146313\n",
      "[78]\ttraining's binary_error: 0.00409889\tvalid_1's binary_error: 0.146313\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's binary_error: 0.0187012\tvalid_1's binary_error: 0.145161\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "T/F Classifier\n",
    "'''\n",
    "label_index=2\n",
    "d_train = lgb.Dataset(train_X,train_Y[:,label_index])\n",
    "d_valid = lgb.Dataset(test_X,test_Y[:,label_index])\n",
    "\n",
    "watch_list = [d_train,d_valid]\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',\n",
    "        #'num_class' : 16,\n",
    "        'num_leaves': 32,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'nthread': 4\n",
    "    }\n",
    "#evals_result={}\n",
    "model3 = lgb.train(params, d_train, 100, watch_list, early_stopping_rounds=15,verbose_eval=1) # 65\n",
    "#online_model3 = lgb.train(params,lgb.Dataset(train_X,train_Y[:,label_index]),num_boost_round=67) # 这里写个300-400效果更好，为了速度问题写的100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868,)\n",
      "0.869022869022869\n",
      "0.9252843291122912\n"
     ]
    }
   ],
   "source": [
    "preds=model3.predict(test_X)\n",
    "print(preds.shape)\n",
    "print(f1_score(test_Y[:,label_index],preds>0.5))\n",
    "print(roc_auc_score(test_Y[:,label_index],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_error: 0.397464\tvalid_1's binary_error: 0.381336\n",
      "Training until validation scores don't improve for 15 rounds.\n",
      "[2]\ttraining's binary_error: 0.397464\tvalid_1's binary_error: 0.381336\n",
      "[3]\ttraining's binary_error: 0.225823\tvalid_1's binary_error: 0.229263\n",
      "[4]\ttraining's binary_error: 0.1938\tvalid_1's binary_error: 0.205069\n",
      "[5]\ttraining's binary_error: 0.17843\tvalid_1's binary_error: 0.197005\n",
      "[6]\ttraining's binary_error: 0.170744\tvalid_1's binary_error: 0.190092\n",
      "[7]\ttraining's binary_error: 0.161009\tvalid_1's binary_error: 0.191244\n",
      "[8]\ttraining's binary_error: 0.157679\tvalid_1's binary_error: 0.193548\n",
      "[9]\ttraining's binary_error: 0.153964\tvalid_1's binary_error: 0.191244\n",
      "[10]\ttraining's binary_error: 0.151018\tvalid_1's binary_error: 0.192396\n",
      "[11]\ttraining's binary_error: 0.144101\tvalid_1's binary_error: 0.187788\n",
      "[12]\ttraining's binary_error: 0.142692\tvalid_1's binary_error: 0.195853\n",
      "[13]\ttraining's binary_error: 0.139362\tvalid_1's binary_error: 0.193548\n",
      "[14]\ttraining's binary_error: 0.137697\tvalid_1's binary_error: 0.193548\n",
      "[15]\ttraining's binary_error: 0.1368\tvalid_1's binary_error: 0.193548\n",
      "[16]\ttraining's binary_error: 0.134879\tvalid_1's binary_error: 0.193548\n",
      "[17]\ttraining's binary_error: 0.130908\tvalid_1's binary_error: 0.197005\n",
      "[18]\ttraining's binary_error: 0.128987\tvalid_1's binary_error: 0.201613\n",
      "[19]\ttraining's binary_error: 0.127706\tvalid_1's binary_error: 0.199309\n",
      "[20]\ttraining's binary_error: 0.124119\tvalid_1's binary_error: 0.198157\n",
      "[21]\ttraining's binary_error: 0.121942\tvalid_1's binary_error: 0.197005\n",
      "[22]\ttraining's binary_error: 0.119764\tvalid_1's binary_error: 0.197005\n",
      "[23]\ttraining's binary_error: 0.117331\tvalid_1's binary_error: 0.191244\n",
      "[24]\ttraining's binary_error: 0.115281\tvalid_1's binary_error: 0.195853\n",
      "[25]\ttraining's binary_error: 0.112207\tvalid_1's binary_error: 0.193548\n",
      "[26]\ttraining's binary_error: 0.112079\tvalid_1's binary_error: 0.1947\n",
      "Early stopping, best iteration is:\n",
      "[11]\ttraining's binary_error: 0.144101\tvalid_1's binary_error: 0.187788\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "J/P Classifier\n",
    "'''\n",
    "label_index=3\n",
    "d_train = lgb.Dataset(train_X,train_Y[:,label_index])\n",
    "d_valid = lgb.Dataset(test_X,test_Y[:,label_index])\n",
    "\n",
    "watch_list = [d_train,d_valid]\n",
    "\n",
    "params = {\n",
    "        'learning_rate': 0.1,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',\n",
    "        #'num_class' : 16,\n",
    "        'num_leaves': 32,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'nthread': 4\n",
    "    }\n",
    "#evals_result={}\n",
    "model4 = lgb.train(params, d_train, 100, watch_list, early_stopping_rounds=15,verbose_eval=1) #30\n",
    "#online_model4 = lgb.train(params,lgb.Dataset(train_X,train_Y[:,label_index]),num_boost_round=33) # 这里写个300-400效果更好，为了速度问题写的100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(868,)\n",
      "0.8566402814423921\n",
      "0.8803130291931792\n"
     ]
    }
   ],
   "source": [
    "preds=model4.predict(test_X)\n",
    "print(preds.shape)\n",
    "print(f1_score(test_Y[:,label_index],preds>0.5))\n",
    "print(roc_auc_score(test_Y[:,label_index],preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lgb/lgb_clfs.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump([online_model1,online_model2,online_model3,online_model4],'lgb/lgb_clfs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the chat data with MBTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2300066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"there's nothing to tell! he's just some guy i work with!\",\n",
       " \"c'mon, you're going out with the guy! there's gotta be something wrong with him!\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data=joblib.load('../../data/chat/chat_data.pkl')\n",
    "print(len(chat_data))\n",
    "chat_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_word(posts,remove_stop_words=True):\n",
    "    temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', posts)\n",
    "    temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "    temp = re.sub(' +', ' ', temp).lower()\n",
    "    if remove_stop_words:\n",
    "        temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
    "    else:\n",
    "        temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(word):\n",
    "    X_tfidf = tfidf.transform(word)\n",
    "    X_ha = ha.transform(word)\n",
    "    X=hstack([X_tfidf,X_ha])\n",
    "    proba=[]\n",
    "    for clf in [online_model1, online_model2, online_model3, online_model4]:\n",
    "        proba.append(clf.predict(X)[0])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.02560997804005941\n",
      "2 0.053137771288553876\n",
      "3 0.07807209094365437\n",
      "4 0.10307408968607584\n",
      "5 0.1286808451016744\n",
      "6 0.1560165246327718\n",
      "7 0.19099481105804444\n",
      "8 0.22548333406448365\n",
      "9 0.26292581160863243\n",
      "10 0.2971615513165792\n",
      "11 0.3313321590423584\n",
      "12 0.36735142866770426\n",
      "13 0.4033103426297506\n",
      "14 0.43987718820571897\n",
      "15 0.4742065866788228\n",
      "16 0.5090662956237793\n",
      "17 0.545743179321289\n",
      "18 0.5774400234222412\n",
      "19 0.612158203125\n",
      "20 0.6460737347602844\n",
      "21 0.6803745269775391\n",
      "22 0.7136312166849772\n",
      "23 0.7483024636904398\n",
      "24 0.781859294573466\n",
      "25 0.8168975790341695\n",
      "26 0.8510376652081807\n",
      "27 0.8860677798589071\n",
      "28 0.9233596801757813\n",
      "29 0.9572187662124634\n",
      "30 0.9910502235094706\n",
      "31 1.0242370645205179\n",
      "32 1.0572924375534059\n",
      "33 1.0914519508679708\n",
      "34 1.1254964311917623\n",
      "35 1.1598407824834187\n",
      "36 1.1955721855163575\n",
      "37 1.2318145791689554\n",
      "38 1.2689782818158468\n",
      "39 1.3054359078407287\n",
      "40 1.340715487798055\n",
      "41 1.3790478189786275\n",
      "42 1.412366251150767\n",
      "43 1.4477190335591634\n",
      "44 1.482447592417399\n",
      "45 1.5169925888379414\n",
      "46 1.5523545225461324\n",
      "47 1.5862053791681925\n",
      "48 1.6218311111132304\n",
      "49 1.6560471216837565\n",
      "50 1.6916335066159567\n",
      "51 1.7266441226005553\n",
      "52 1.761935571829478\n",
      "53 1.7970485925674438\n",
      "54 1.8319513519605002\n",
      "55 1.8671759605407714\n",
      "56 1.9015268325805663\n",
      "57 1.9424241185188293\n",
      "58 1.978469439347585\n",
      "59 2.013139907519023\n",
      "60 2.048667867978414\n",
      "61 2.0854989926020306\n",
      "62 2.12000124057134\n",
      "63 2.1563576539357503\n",
      "64 2.189776833852132\n",
      "65 2.2243661403656008\n",
      "66 2.259378425280253\n",
      "67 2.2923332691192626\n",
      "68 2.327495817343394\n",
      "69 2.3613627115885416\n",
      "70 2.398058803876241\n",
      "71 2.433880388736725\n",
      "72 2.4705970168113707\n",
      "73 2.507031261920929\n",
      "74 2.5432459195454915\n",
      "75 2.5795672059059145\n",
      "76 2.6147424538930255\n",
      "77 2.648529287179311\n",
      "78 2.688585146268209\n",
      "79 2.723715813954671\n",
      "80 2.7571642637252807\n",
      "81 2.7913650194803874\n",
      "82 2.825029389063517\n",
      "83 2.859624747435252\n",
      "84 2.8935911615689593\n",
      "85 2.9265899777412416\n",
      "86 2.96139079729716\n",
      "87 2.9961997191111247\n",
      "88 3.0333216150601703\n",
      "89 3.0698707501093545\n",
      "90 3.1037763794263205\n",
      "91 3.1385964234670003\n",
      "92 3.1743746479352315\n",
      "93 3.2086049914360046\n",
      "94 3.2434388756752015\n",
      "95 3.2782251715660093\n",
      "96 3.308419108390808\n",
      "97 3.343640764554342\n",
      "98 3.3788789669672648\n",
      "99 3.412463692824046\n",
      "100 3.4481603026390077\n",
      "101 3.484281329313914\n",
      "102 3.519275446732839\n",
      "103 3.560468876361847\n",
      "104 3.5908108711242677\n",
      "105 3.624172047773997\n",
      "106 3.6594581961631776\n",
      "107 3.6930694500605266\n",
      "108 3.727727202574412\n",
      "109 3.7613900780677794\n",
      "110 3.7959072391192117\n",
      "111 3.8314979712168378\n",
      "112 3.8660513758659363\n",
      "113 3.9010373830795286\n",
      "114 3.9362338463465374\n",
      "115 3.9720433195432028\n",
      "116 4.007135419050853\n",
      "117 4.041873586177826\n",
      "118 4.0768095771471655\n",
      "119 4.111917034784953\n",
      "120 4.149323769410452\n",
      "121 4.183693095048269\n",
      "122 4.217247140407562\n",
      "123 4.2528074542681376\n",
      "124 4.287499149640401\n",
      "125 4.32063315709432\n",
      "126 4.35562801361084\n",
      "127 4.388768025239309\n",
      "128 4.423180409272512\n",
      "129 4.456588864326477\n",
      "130 4.491070751349131\n",
      "131 4.523944183190664\n",
      "132 4.558415079116822\n",
      "133 4.591096683343252\n",
      "134 4.626592063903809\n",
      "135 4.66990579366684\n",
      "136 4.704831941922506\n",
      "137 4.740942370891571\n",
      "138 4.775538611412048\n",
      "139 4.807638700803121\n",
      "140 4.84252389272054\n",
      "141 4.8754456837972\n",
      "142 4.9084024349848425\n",
      "143 4.94565574725469\n",
      "144 4.980307722091675\n",
      "145 5.014638006687164\n",
      "146 5.0494996031125385\n",
      "147 5.0831782142321265\n",
      "148 5.118542619546255\n",
      "149 5.153590762615204\n",
      "150 5.189371037483215\n",
      "151 5.224620521068573\n",
      "152 5.259616649150848\n",
      "153 5.293307157357534\n",
      "154 5.324243521690368\n",
      "155 5.3569899797439575\n",
      "156 5.3969271898269655\n",
      "157 5.438264532883962\n",
      "158 5.475261398156484\n",
      "159 5.511189583937327\n",
      "160 5.549207667509715\n",
      "161 5.58619179725647\n",
      "162 5.6224085251490274\n",
      "163 5.660144289334615\n",
      "164 5.6972322503725685\n",
      "165 5.732491091887156\n",
      "166 5.770272362232208\n",
      "167 5.805645708243052\n",
      "168 5.842528736591339\n",
      "169 5.879951930046081\n",
      "170 5.920645689964294\n",
      "171 5.956875455379486\n",
      "172 5.9910100181897485\n",
      "173 6.027467147509257\n",
      "174 6.062910540898641\n",
      "175 6.1066492080688475\n",
      "176 6.142085337638855\n",
      "177 6.1762498418490095\n",
      "178 6.2126154661178585\n",
      "179 6.247941323121389\n",
      "180 6.28337097565333\n",
      "181 6.318643271923065\n",
      "182 6.3540369987487795\n",
      "183 6.389536873499552\n",
      "184 6.425208771228791\n",
      "185 6.461362485090891\n",
      "186 6.495450727144877\n",
      "187 6.531170173486074\n",
      "188 6.566539319356282\n",
      "189 6.6003523707389835\n",
      "190 6.634765549500783\n",
      "191 6.67032128572464\n",
      "192 6.704708762963613\n",
      "193 6.740103185176849\n",
      "194 6.774839448928833\n",
      "195 6.810485994815826\n",
      "196 6.8446765542030334\n",
      "197 6.880255893866221\n",
      "198 6.91476008494695\n",
      "199 6.949515342712402\n",
      "200 6.983785756429037\n",
      "201 7.016476730505626\n",
      "202 7.051621715227763\n",
      "203 7.08524196545283\n",
      "204 7.119988791147867\n",
      "205 7.153679911295573\n",
      "206 7.188465841611227\n",
      "207 7.223122950394949\n",
      "208 7.258608635266622\n",
      "209 7.293309541543325\n",
      "210 7.328476965427399\n",
      "211 7.362789344787598\n",
      "212 7.397808305422465\n",
      "213 7.432978165149689\n",
      "214 7.465058219432831\n",
      "215 7.498446647326151\n",
      "216 7.532421239217123\n",
      "217 7.566071097056071\n",
      "218 7.60077900091807\n",
      "219 7.634367581208547\n",
      "220 7.671258429686229\n",
      "221 7.705340087413788\n",
      "222 7.7346789836883545\n",
      "223 7.763187777996063\n",
      "224 7.8027208089828495\n",
      "225 7.832099358240764\n",
      "226 7.861047645409902\n",
      "227 7.8938880443573\n",
      "228 7.922333963712057\n",
      "229 7.951229826609294\n",
      "230 7.979466736316681\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t1=time()\n",
    "label_data=[]\n",
    "batch_size=10000\n",
    "batch_start=0\n",
    "batch_end=batch_start+batch_size\n",
    "fuck=0\n",
    "while batch_end < len(chat_data):\n",
    "    chunck_data=chat_data[batch_start:batch_end]\n",
    "    ques=[];ans=[];temp=[];\n",
    "    for dat in chunck_data:\n",
    "        ques.append(dat[0])\n",
    "        ans.append(dat[1])\n",
    "        temp.append(sub_word(dat[1]))\n",
    "    proba=get_proba(temp)\n",
    "    new_line=[]\n",
    "    for ii in range(len(temp)):\n",
    "        new_line.append((ques[ii],ans[ii],[proba[0][ii],proba[1][ii],proba[2][ii],proba[3][ii]]))\n",
    "    label_data.extend(new_line)\n",
    "    batch_start+=batch_size\n",
    "    batch_end+=batch_size\n",
    "    fuck+=1\n",
    "    print(fuck,(time()- t1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/chat/labeled_chat_data.pkl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(label_data,'../../data/chat/labeled_chat_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750000 ('random pic of the day-doctor who-last christmas , 2014 ', \"the term last christmas has become a staple point to our christmases . we're more attentive when we see each other each year . \", [0.12544562876859472, 0.3110942006878408, 0.5935377123624724, 0.6965982725652208])\n"
     ]
    }
   ],
   "source": [
    "sampled_data=[]\n",
    "for ind in choise_inds:\n",
    "    sampled_data.append(label_data[ind])\n",
    "print(len(sampled_data),sampled_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/chat/sampled_chat_data.pkl']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sampled_data,'../../data/chat/sampled_chat_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBTI Discrimator API with LGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBTI():\n",
    "    def __init__(self, pkl_dir='lgb/'):\n",
    "        super(MBTI, self).__init__()\n",
    "        self.models = joblib.load(os.path.join(pkl_dir,'lgb_clfs.pkl'))\n",
    "        self.ie_clf=self.models[0]\n",
    "        self.ns_clf=self.models[1]\n",
    "        self.tf_clf=self.models[2]\n",
    "        self.jp_clf=self.models[3]\n",
    "        self.tfidf =  joblib.load(os.path.join(pkl_dir,'tfidf.pkl'))\n",
    "        self.ha =  joblib.load(os.path.join(pkl_dir,'ha.pkl'))\n",
    "        \n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatiser = WordNetLemmatizer()\n",
    "        self.cachedStopWords = stopwords.words(\"english\")\n",
    "        \n",
    "        print('Successfully load the model...')\n",
    "    \n",
    "    \n",
    "    def _sub_word(self,posts,remove_stop_words=True):\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', posts)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp).lower()\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([self.lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in self.cachedStopWords])\n",
    "        else:\n",
    "            temp = \" \".join([self.lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "        return temp\n",
    "    \n",
    "    def _get_feature(self,raw_x):\n",
    "        processed=[]\n",
    "        for item in raw_x:\n",
    "            processed.append(self._sub_word(item))\n",
    "            \n",
    "        X_tfidf = self.tfidf.transform(processed)\n",
    "        X_ha = self.ha.transform(processed)\n",
    "        X=hstack([X_tfidf,X_ha])\n",
    "        return X\n",
    "    \n",
    "    @property\n",
    "    def label_name(self):\n",
    "        lab_name = ['Introversion (I) – Extroversion (E)',\n",
    "                    'Intuition (N) – Sensing (S)',\n",
    "                    'Thinking (T) – Feeling (F)',\n",
    "                    'Judging (J) – Perceiving (P)']\n",
    "        return lab_name\n",
    "        \n",
    "    def predict(self, x):\n",
    "        feat=self._get_feature(x)\n",
    "        ie=self.ie_clf.predict(feat)\n",
    "        ns=self.ns_clf.predict(feat)\n",
    "        tf=self.tf_clf.predict(feat)\n",
    "        jp=self.jp_clf.predict(feat)\n",
    "        \n",
    "        proba=[]\n",
    "        for i,n,t,j in zip(ie,ns,tf,jp):\n",
    "            proba.append([i,n,t,j])\n",
    "            \n",
    "        return np.array(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=MBTI()\n",
    "print(mbti.label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti=MBTI()\n",
    "x=['hello what is your name?',\n",
    "   'My name is your fathre',\n",
    "  'haha'\n",
    "  ]\n",
    "preds=mbti.predict(x)\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
